{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Du7mh8MN3rSs"
      },
      "outputs": [],
      "source": [
        "!pip install gradio huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "-UIDm-A23x7D",
        "outputId": "d5b7c01f-01ee-4976-f02d-445c528165c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "/tmp/ipython-input-2456631038.py:135: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
            "/tmp/ipython-input-2456631038.py:140: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7631f7908f01c65a54.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7631f7908f01c65a54.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import json\n",
        "from google.colab import userdata # Import userdata to access Colab secrets\n",
        "\n",
        "# Define the directory where the model and associated files are saved\n",
        "# Changed from Google Drive path to a local Colab path\n",
        "output_dir = '/content/model'\n",
        "\n",
        "# Load the tokenizer from the specified directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir, use_fast=False)\n",
        "\n",
        "# Load label mappings from JSON files\n",
        "with open(f\"{output_dir}/label_map.json\", \"r\") as f:\n",
        "    label_map = json.load(f)\n",
        "with open(f\"{output_dir}/reverse_label_map.json\", \"r\") as f:\n",
        "    reverse_label_map = {int(k): v for k, v in json.load(f).items()} # Convert keys to int\n",
        "\n",
        "# Determine the number of labels for model configuration\n",
        "num_labels = len(label_map)\n",
        "\n",
        "# Load the fine-tuned sequence classification model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(output_dir, num_labels=num_labels)\n",
        "\n",
        "# Move the model to the appropriate device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Initialize pre-trained sentiment and emotion analysis pipelines\n",
        "sentiment_analyzer = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest', device=0 if torch.cuda.is_available() else -1)\n",
        "emotion_analyzer = pipeline('text-classification', model='SamLowe/roberta-base-go_emotions', top_k=None, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "\n",
        "def perform_all_analyses(input_text):\n",
        "    \"\"\"\n",
        "    Performs mental health classification, sentiment analysis, and emotion analysis\n",
        "    on a given input text.\n",
        "    \"\"\"\n",
        "    # Tokenize input for the mental health classification model\n",
        "    encoded_input = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "    # Move inputs to the designated device\n",
        "    input_ids = encoded_input['input_ids'].to(device)\n",
        "    attention_mask = encoded_input['attention_mask'].to(device)\n",
        "\n",
        "    # Perform mental health prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, axis=-1).item()\n",
        "    mental_health_label = reverse_label_map[prediction]\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    sentiment_result = sentiment_analyzer(input_text)[0]\n",
        "\n",
        "    # Perform emotion analysis and get top 3 emotions\n",
        "    emotion_raw_results = emotion_analyzer(input_text)[0]\n",
        "    sorted_emotions = sorted(emotion_raw_results, key=lambda x: x['score'], reverse=True)\n",
        "    top_emotions = sorted_emotions[:3]\n",
        "\n",
        "    # Return all analysis results\n",
        "    return {\n",
        "        'mental_health_prediction': mental_health_label,\n",
        "        'sentiment_analysis': {\n",
        "            'label': sentiment_result['label'],\n",
        "            'score': sentiment_result['score']\n",
        "        },\n",
        "        'emotion_analysis': top_emotions\n",
        "    }\n",
        "\n",
        "# Setup the Generative Model (The \"Voice\" of the bot)\n",
        "# Retrieve the token from Colab secrets. Make sure you've saved your Hugging Face token under 'HF_TOKEN'.\n",
        "HUGGING_FACE_TOKEN = userdata.get('HF_TOKEN')\n",
        "client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\", token=HUGGING_FACE_TOKEN)\n",
        "\n",
        "def generate_smart_response(user_text, analysis_result):\n",
        "    \"\"\"\n",
        "    Generates a human-like, empathetic response based on analysis results.\n",
        "    \"\"\"\n",
        "    status = analysis_result['mental_health_prediction']\n",
        "    sentiment = analysis_result['sentiment_analysis']['label']\n",
        "    primary_emotion = analysis_result['emotion_analysis'][0]['label']\n",
        "    top_emotions_str = \", \".join([f\"{e['label']} (score: {e['score']:.2f})\" for e in analysis_result['emotion_analysis'][:3]])\n",
        "\n",
        "    # Conditional guidance for the LLM based on perceived severity\n",
        "    severity_guidance = \"\"\n",
        "    if status in ['depression', 'anxiety', 'burnout', 'stress']:\n",
        "        severity_guidance = (\n",
        "            \"Given the nature of their feelings, gently suggest exploring ways to support their well-being, \"\n",
        "            \"such as talking to someone they trust, engaging in self-care, or seeking professional guidance. \"\n",
        "            \"Integrate this suggestion naturally into your response.\"+ # Concatenated string for better formatting\n",
        "            \"Keep the response short (under 3 sentences) and supportive.\"\n",
        "        )\n",
        "\n",
        "    # Construct the prompt for the LLM\n",
        "    system_prompt = (\n",
        "        f\"You are a compassionate mental health support assistant. \"\n",
        "        f\"The user just said: '{user_text}'. \"\n",
        "        f\"Based on an internal analysis, I've identified: \"\n",
        "        f\"- Mental state: '{status}'\\n\"\n",
        "        f\"- Overall sentiment: '{sentiment}'\\n\"\n",
        "        f\"- Top emotions: {top_emotions_str}. \"\n",
        "        f\"Please respond in a warm, validating, and human-like way. \"\n",
        "        f\"First, empathetically acknowledge these insights (mental state, sentiment, and primary emotion) directly. \"\n",
        "        f\"Then, offer supportive words and a practical suggestion relevant to their mental state. \"\n",
        "        f\"Keep your response concise, around 2-4 sentences, combining acknowledgment and support naturally. \"\n",
        "        f\"{severity_guidance}\"\n",
        "    )\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_text}]\n",
        "\n",
        "    # Generate response using the inference client\n",
        "    response = \"\"\n",
        "    for message in client.chat_completion(messages, max_tokens=150, stream=True):\n",
        "        token = message.choices[0].delta.content\n",
        "        response += token\n",
        "\n",
        "    return response\n",
        "\n",
        "def chat_logic(message, history):\n",
        "    \"\"\"\n",
        "    Main chat logic: analyzes user input and generates a bot response.\n",
        "    \"\"\"\n",
        "    # Run backend analysis on the user message\n",
        "    raw_analysis = perform_all_analyses(message)\n",
        "\n",
        "    # Generate a human-like response\n",
        "    bot_response = generate_smart_response(message, raw_analysis)\n",
        "\n",
        "    return bot_response\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Mental Health Companion\")\n",
        "    gr.Markdown(\"I'm here to listen. Tell me how you are feeling today.\")\n",
        "\n",
        "    # Initialize the chatbot component with an initial bot message\n",
        "    chatbot = gr.Chatbot(\n",
        "        value=[{\"role\": \"assistant\", \"content\": \"Hello. I'm here to support you. How are you feeling right now?\"}],\n",
        "        height=400,\n",
        "        type=\"messages\"\n",
        "    )\n",
        "\n",
        "    msg = gr.Textbox(placeholder=\"Type your feelings here...\", show_label=False)\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    def user_interaction(user_message, history):\n",
        "        # Append user message to history and clear input box\n",
        "        history.append({\"role\": \"user\", \"content\": user_message})\n",
        "        return \"\", history\n",
        "\n",
        "    def bot_interaction(history):\n",
        "        # Get the last user message from history\n",
        "        user_message = history[-1]['content']\n",
        "\n",
        "        # Get bot response\n",
        "        response_text = chat_logic(user_message, history)\n",
        "\n",
        "        # Append bot response to history\n",
        "        history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "        return history\n",
        "\n",
        "    # Configure chat interaction flow\n",
        "    msg.submit(user_interaction, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        bot_interaction, [chatbot], [chatbot]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio application\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
